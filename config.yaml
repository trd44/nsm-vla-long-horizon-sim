# config.yaml
vlm_agent:
  api_key: "your_api_key"
  model: "o1-preview"
  temperature: 0.5
# seting for image observation
image_path: "images/agent_view.jpg"

execution_dir: "execution"
learning_dir: "learning"
detection_dir: "detection"

plan_learn_execute:
  max_iter: 100

simulation:
  robots: "Kinova3"
  has_renderer: False
  has_offscreen_renderer: False
  use_camera_obs: False
  control_freq: 20
  reward_shaping: True

# simulation environment setup for the evaluation of the RL policy during training. Parameters are the same as the simulation environment setup for the RL training above except for `has_renderer`
eval_simulation:
  robots: "Kinova3"
  has_renderer: True
  has_offscreen_renderer: False
  use_camera_obs: False
  control_freq: 20
  reward_shaping: True

# parameters for RL
learning:
  model:
    batch_size: 256 # default
    buffer_size: 1000000
    gamma: 0.99 # default
    learning_rate: 0.0003
    learning_starts: 10000 # default 10000
    verbose: 1
    policy_kwargs:
      net_arch: [512, 512, 256]
    seed: 0
    tau: 0.005 # default
  eval:
    eval_freq: 10000 # default 10000
    n_eval_episodes: 5
    deterministic: True
    render: False # if True, make sure `render_mode` below is set up "human" and `eval_simulation` above is set up with `has_renderer` as True
    render_mode: "human"
    verbose: 1
  learn:
    total_timesteps: 5000000
  
# setting for testing PDDL. Uncoment when testing.

# planning:
#   planning_dir: "planning/PDDL/coffee"
#   domain: "coffee"
#   init_planning_domain: "coffee_domain.pddl" # the initial domain file
#   modified_planning_domain: "modified_coffee_domain.pddl" # final domain file will be saved as this
#   planning_problem: "coffee_problem.pddl" # the partially defined planning problem file
#   planning_goal_node: "goal_node" # the pickled file name for the goal node
#   novel_objects: ["drawer1"]
#   num_op_candidates: 5
#   max_new_operators_branching_factor: 1 # prompt the llm this many times in each state for a new operator
#   max_num_llm_calls: 60
#   max_depth: 15 # maximum search depth
#   min_plan_candidates: 1 # end search early if this many plans are found

# planning:
#   planning_dir: "planning/PDDL/cleanup"
#   domain: "cleanup"
#   init_planning_domain: "cleanup_domain.pddl" # the initial domain file
#   modified_planning_domain: "modified_cleanup_domain.pddl" # final domain file will be saved as this
#   planning_problem: "cleanup_problem.pddl" # the partially defined planning problem file
#   planning_goal_node: "goal_node" # the pickled file name for the goal node
#   novel_objects: ["mug1"]
#   num_op_candidates: 5
#   max_new_operators_branching_factor: 1 # prompt the llm this many times in each state for a new operator
#   max_num_llm_calls: 30
#   max_depth: 15 # maximum search depth
#   min_plan_candidates: 1 # end search early if this many plans are found

planning:
  planning_dir: "planning/PDDL/nut_assembly"
  domain: "nut_assembly"
  init_planning_domain: "nut_assembly_domain.pddl" # the initial domain file
  modified_planning_domain: "modified_nut_assembly_domain.pddl" # final domain file will be saved as this
  planning_problem: "nut_assembly_problem.pddl" # the partially defined planning problem file
  planning_goal_node: "goal_node" # the pickled file name for the goal node
  novel_objects: ["round-peg1"]
  num_op_candidates: 5
  max_new_operators_branching_factor: 1 # prompt the llm this many times in each state for a new operator
  max_num_llm_calls: 30
  max_depth: 4 # maximum search depth
  min_plan_candidates: 1 # end search early if this many plans are found


logging: # TODO: enable this
  level: "INFO"
  file: "vlm_agent.log"
