# config.yaml
vlm_agent:
  model: "gpt-4o"
  temperature: 0.0
llm_agent:
  model: "o3-mini" 
  temperature: 0.0
# seting for image observation
image_path: "images/agent_view.jpg"

execution_dir: "execution"
learning_dir: "learning"
detection_dir: "detection"

plan_learn_execute:
  max_iter: 100

simulation:
  robots: "Kinova3"
  has_renderer: True
  has_offscreen_renderer: False
  use_camera_obs: False
  control_freq: 20
  reward_shaping: True

# simulation environment setup for the evaluation of the RL policy during training. Parameters are the same as the simulation environment setup for the RL training above except for `has_renderer`
eval_simulation:
  robots: "Kinova3"
  has_renderer: True
  has_offscreen_renderer: False
  use_camera_obs: False
  control_freq: 20
  reward_shaping: True

# parameters for RL
learning:
  PPO:
    batch_size: 64 # default 64. The n_steps taken per VectorEnv will be divided into `batch_size` minibatches for training the policy
    gamma: 0.99 # default
    learning_rate: 0.0003 # default
    verbose: 1
    n_steps: 2048 #default 2048, make sure this is smaller than or equal to `timesteps_per_iter` below. Otherwise this will take precedence. The agent takes `n_steps` steps in the environment per VectorEnv before updating the policy.
    policy_kwargs:
      net_arch:
        pi: [64, 64] # default [64, 64]
        vf: [64, 64] # default [64, 64]
    seed: 0 # 0 by default unless overwriten by command line argument
  DDPG:
    batch_size: 100 # default
    buffer_size: 1000000 # default
    gamma: 0.99 # default
    learning_rate: 0.001 # default
    learning_starts: 100 # default
    verbose: 1
    policy_kwargs:
      net_arch: [400, 300] # default [400, 300]
    tau: 0.005 # default
    action_noise: "OrnsteinUhlenbeckActionNoise" # default
    action_noise_kwargs:
      mean: 0.0
      sigma: 0.2
      theta: 0.15
      dt: 0.01
    seed: 0 # 0 by default unless overwriten by command line argument
  SAC:
    batch_size: 256 # default
    buffer_size: 1000000 # not for PPO
    gamma: 0.99 # default
    learning_rate: 0.0003
    learning_starts: 10000 # default 10000
    verbose: 1
    policy_kwargs:
      net_arch: [256, 256] # [256, 256]
    tau: 0.005 # default
    seed: 0 # 0 by default unless overwriten by command line argument
  eval:
    eval_freq: 10000
    n_eval_episodes: 5
    deterministic: True
    render: False # if True, make sure `render_mode` below is set to "human" and `eval_simulation` above is set up with `has_renderer` as True
    render_mode: "human"
    verbose: 1
  learn_operator:
    total_timesteps: 5000000
  learn_subgoal:
    total_timesteps: 1000000
    timesteps_per_iter: 50000
  reward_shaping_fn:
    num_candidates: 3
  

planning:
  hanoi:
    planning_dir: "planning/PDDL/hanoi"
    domain: "hanoi"
    init_planning_domain: "domain.pddl" # the initial domain file
    planning_problem: "problem_dummy.pddl" # the partially defined planning problem file
    planning_goal_node: "goal_node" # the pickled file name for the goal node
    max_depth: 15 # maximum search depth
  kitchen:
    planning_dir: "planning/PDDL/kitchen"
    domain: "kitchen"
    init_planning_domain: "domain.pddl" # the initial domain file
    planning_problem: "problem_dummy.pddl" # the partially defined planning problem file
    planning_goal_node: "goal_node" # the pickled file name for the goal node
    max_depth: 15 # maximum search depth
  nut_assembly:
    planning_dir: "planning/PDDL/nut_assembly"
    domain: "nut_assembly"
    init_planning_domain: "domain.pddl" # the initial domain file
    planning_problem: "problem_dummy.pddl" # the partially defined planning problem file
    planning_goal_node: "goal_node" # the pickled file name for the goal nod
    max_depth: 15 # maximum search depth
  sorting:
    planning_dir: "planning/PDDL/sorting"
    domain: "sorting"
    init_planning_domain: "domain.pddl" # the initial domain file
    planning_problem: "problem_dummy.pddl" # the partially defined planning problem file
    planning_goal_node: "goal_node" # the pickled file name for the goal nod
    max_depth: 15 # maximum search depth

logging: # TODO: enable this
  level: "INFO"
  file: "vlm_agent.log"
